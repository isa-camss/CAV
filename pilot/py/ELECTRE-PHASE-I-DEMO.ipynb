{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">ELECTRE METHOD - PHASE I SHOW</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>This notebook illustrates how the <b>ELECTRE-II</b> method is used to rank the best tenders for one Procurement Procedure Lot.</h3>\n",
    "\n",
    "The algorithm followed for this goes as follows:\n",
    "\n",
    "1. Extract data from an Excel file containing the comparative values assigned by 3 evaluators to a set of Quality AwardCriteria\n",
    "2. Transform the extracted data into a Graph compliant with the CAV-Anticorrupzione-Application Profile TBox \n",
    "3. Load the data in a Graph Store\n",
    "4. Query the Graph Store to retrieve the data\n",
    "5. Rank the tender-lots\n",
    "6. Visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl\n",
    "import sparql as q\n",
    "import store\n",
    "import electre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step #1: Extraction of the data from the excel file</h2>\n",
    "<u><b>Note:</b></u>\n",
    "\n",
    "If you want to change this file download it from <a href=\"./evaluation-tool.xlsm\">here</a>, modify it and save it in the root directory of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCEL_FILE = 'evaluation-tool.xlsm'\n",
    "DATA_TAB = 'REPORTS PER EVALUATOR'\n",
    "CRITERIA_TAB = 'MAIN REPORT'\n",
    "g = etl.graph()  # Creates the graph of the ABox\n",
    "g, criteria = etl.instantiate_criteria(g, EXCEL_FILE, CRITERIA_TAB)\n",
    "e = etl.extract(EXCEL_FILE, DATA_TAB)           # Creates a pandas data frame from the Excel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Here you have an overview of the data in a pandas' data frame:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step #2: Creation of the CAV-AC-AP ABox</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, r = etl.build_roles(g)                       # Instantiates the role 'evaluator'\n",
    "g, tl = etl.lots_tendered(g)                    # Instantiates TenderLots 'A' to 'L'\n",
    "g = etl.transform(e, g, tl, r, criteria)        # Populates the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The results have been extracted into a file named 'a-box.ttl'. Here you have the contents (it's a long process, since it generates many triples, so you will have to wait a short while):</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.show_save(g, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step #3: Store the graph into the Graph Store</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We first connect to the Graph Store.</h4>\n",
    "<p><b><u>Note</u></b>: This show uses a local GraphDB store and a repository named 'cav-pilot'. In your case you will have to have a Graph Store available, either locally or remotely, and re-configure the code (see the function 'connect()' inside the \n",
    "module 'store.py'.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = store.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We then drop the current 'named graph', if any.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.drop(sparql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we load the graph into the Graph Store (again, many triples are to be inserted, so wait a bit, please...)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.insert(g, sparql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step #4: Query the Graph Store and retrieve the data per criterion, evaluator, etc.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.1. Let us retrieve now the data from the Graph Store.</h4> \n",
    "\n",
    "4.1.1. The <b><u>User Story</u></b> for this operation goes as this:\n",
    "\n",
    "<p><div align=\"center\"><i>I, as the algorithm calculating and ranking which is the winner of Lot1 of the procurement procedure X, want to get all input values introduced by all evaluators for this procedure's lot and concerning exclusively 'Qualitative Subjective Criteria', so I can assess the deviations between the evaluators' decisions.</i></div></p>\n",
    "\n",
    "4.1.2. The <b><u>Competency Question</u></b> goes like this:\n",
    "<p><div align=\"center\"><i>Retrieve all the input values assigned by all evaluators to 'Qualitative Subjective Criteria', as well as the information related to the input value.</i></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. And the corresponding <b><u>SPARQL Query</u></b> like this:\n",
    "\n",
    "    PREFIX cap: <http://data.europa.eu/w21/2c930c7b-5e2f-4954-8522-bd3411339d6c/extension/cap#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX cccev: <https://data.europe.eu/semanticassets/ns/cv/cccev_v2.0.0#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX ct: <http://data.europa.eu/w21/2c930c7b-5e2f-4954-8522-bd3411339d6c/resource/criterion-type#>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    SELECT \t?input (concat(?givenName, \" \", ?familyName) as ?agent) ?source ?target ?winner ?value (str(?crit) as ?criterion) (str(?cObj) as ?cid) \n",
    "            ?maxscore ?note\n",
    "    FROM cap:\n",
    "    WHERE {\n",
    "        ?input rdf:type cap:InputValue ;\n",
    "           cap:comparesSourceThing ?source ;\n",
    "           cap:comparesWithTargetThing ?target ;\n",
    "           cap:hasBetterCandidate ?winner ;\n",
    "           cap:hasNumericValue ?value ;\n",
    "           cap:isProvidedBy ?agentObj;\n",
    "           cap:refersToCriterion ?cObj .\n",
    "        ?cObj cccev:hasName ?crit ; \n",
    "                   cap:hasMaximumScore ?maxscore ;\n",
    "                   skos:scopeNote ?note .\n",
    "        ?agentObj foaf:givenName ?givenName ;\n",
    "                  foaf:familyName ?familyName .\n",
    "\n",
    "        FILTER EXISTS {?cObj cccev:hasType ct:qs}\n",
    "        FILTER (?source != ?target && ?value != 0)\n",
    "\n",
    "    } ORDER BY ?agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4.2. So, let us execute this SPARQL query...</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = q._query(sparql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 The results are obtained as a ... <b>BIG</b> .. JSON object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step #5: Based on these results, we rank the tender-lots submitted by the tenderers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.1. Let us get the J+ and P+, according to the ELECTRE-II, phase I algorithm:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = electre.get_j(results)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a collection of criteria and per each criterion, it's <u>maximum score</u>, the <u>source tender lot</u> and the <u>target tender-lot</u> represented both with one letter (A-L, without J nor K), the <u>best tender-lot</u> resulting from comparing source and target, it's <u>sign</u> (+, = or -), the name of the <u>evaluator</u>, the <u>value</u> assigned by the evaluator as a result of the tender-pair-evaluation, and its weight (resulting from dividing its value by the maximum score).\n",
    "\n",
    "Next we show you an example for one positive criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'criterion-131d62d76f82880345bfb89aa8d8473b'\n",
    "print('Criterion: {}'.format(criterion.split('-')[1]))\n",
    "print('maximum score: {}'.format(j[criterion]['max_score']))\n",
    "print('source tender-lot: {}'.format(j[criterion]['jw'][4][0]))\n",
    "print('target tender-lot: {}'.format(j[criterion]['jw'][4][1]))\n",
    "print('best tender-lot pair: {}'.format(j[criterion]['jw'][4][2]))\n",
    "print('best tender-lot sign: {}'.format(j[criterion]['jw'][4][3]))\n",
    "print('evaluator: {}'.format(j[criterion]['jw'][4][4]))\n",
    "print('evaluator\\'s assigned value : {}'.format(j[criterion]['jw'][4][5]))\n",
    "print('relative weight (value / max_score) {}'.format(j[criterion]['jw'][4][6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.2. With the individual J+ and P+ we can calculate the summation of each of the tender-lot weights per criteria.</h4>\n",
    "\n",
    "<u><b>Note</b></u>: \n",
    "1. Remember that each tender-lot is represented by a letter from [A, L] without J nor K (do they not exist in Italian?).\n",
    "2. The decimal number is the calculation of the P+ of all the criteria and all evaluators\n",
    "3. Letters not printed out, except for J and K, are P= or P-, we are not interested in them, since P= is assigned a value '1', and P- a value of '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = electre.get_p(j)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.3. Next we can check the <i>concordance</i> indices of each pair</h4>\n",
    "\n",
    "<p>This calculation returns a JSON object with:</p>\n",
    "\n",
    "1. the letter representing the best J+(a,a') tender-lot,\n",
    "2. the decimal being the P+(a, a'), and \n",
    "3. the boolean telling us whether this tender-lot passes the threshold or not (True = pass, False= fails).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the concordance indices c(a, a') = P+(a, a') / P=(a, a') >= c, where c = 3/4 ('natural threshold')\n",
    "# or c = 2/3 ('strong threshold')\n",
    "# The decision of choosing 3/4 or 2/3 is set in the call for tenders.\n",
    "c = 2/3\n",
    "k = electre.get_c(p[0], c)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5.4 Finally we rank the winners, sorted by score</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = electre.rank(k)\n",
    "for key in r:\n",
    "    print('tender-lot: {0} -> score: {1} -> concordance threshold: {2}'.format(\n",
    "        key[0], \n",
    "        round(key[1][0], 2), \n",
    "        'ABOVE (PASSES)' if key[1][1] else 'BELOW'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Visualise the results</h2>\n",
    "\n",
    "<h4>6.1. With these data one can start thinking about how to visualise the data...</h4>\n",
    "\n",
    "e.g. a simple bar chart used to glimpse the differences between the tender-lots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = []\n",
    "y = []\n",
    "threshold = 0.5\n",
    "for key in r:\n",
    "    x.append(key[0])\n",
    "    y.append(round(key[1][0], 2))\n",
    "    \n",
    "plt.bar(x, y)\n",
    "plt.xticks(x)\n",
    "plt.plot([-1, len(x)], [0.5, .5], \"k--\")\n",
    "plt.show()\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2. Similar to the above, but as an area chart:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between( x, y, color=\"skyblue\", alpha=0.2)\n",
    "p = plt.plot(x, y, color=\"Slateblue\", alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3. Other nice-to-have and TODO visualisations...for example:</h4>\n",
    "\n",
    "* Spider chart per Quality Criterion and per tender-lot\n",
    "* Standard deviation chart for all criteria, evaluators and tender-lots\n",
    "* Other ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
